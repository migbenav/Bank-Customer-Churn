{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf157c43-0d1c-4866-a428-e926df1f8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stored variables\n",
    "%store -r X_resampled y_resampled y_test X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c620ff-ebe0-474d-af32-e768b3007b4b",
   "metadata": {},
   "source": [
    "----\n",
    "#### Building a model with Logistic Regression\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f71d2172-6107-4066-a9c7-bb084fd0cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.67      0.77      1593\n",
      "           1       0.36      0.73      0.48       407\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.63      0.70      0.63      2000\n",
      "weighted avg       0.80      0.68      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert the DataFrame and Series to numpy arrays if they were loaded with %store\n",
    "X_resampled = np.array(X_resampled)\n",
    "y_resampled = np.array(y_resampled)\n",
    "y_test = np.array(y_test)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Train the model\n",
    "clf = LogisticRegression(random_state=99, max_iter=500)\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209a424-5392-4159-877b-8a0d8110e44a",
   "metadata": {},
   "source": [
    "----\n",
    "#### Key Observations\n",
    "- High Precision for Non-Churners (0.91). When the model predicts a customer won't churn (0), it's usually correct.\n",
    "- Low Precision for Churners (0.36). When the model predicts churn (1), it's often wrong.\n",
    "- Good Recall for Churners (0.73). The model catches most churners, which is important for retention strategies.\n",
    "- Overall Accuracy is Only 68%\n",
    "- Misclassifying churners could lead to missed retention opportunities\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d684ac81-8fb9-45e0-a92f-e60d3a387acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.55      0.69      1593\n",
      "           1       0.32      0.83      0.46       407\n",
      "\n",
      "    accuracy                           0.61      2000\n",
      "   macro avg       0.62      0.69      0.58      2000\n",
      "weighted avg       0.80      0.61      0.64      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Choosing a new threshold (e.g., where precision & recall are balanced)\n",
    "optimal_threshold = 0.4  \n",
    "\n",
    "# Apply the new threshold\n",
    "y_pred_adjusted = (y_scores >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate again\n",
    "print(classification_report(y_test, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd505b7-cd51-49eb-bcc1-21ed2bbd5d40",
   "metadata": {},
   "source": [
    "----\n",
    "#### Lowering the threshold increases recall, as we capture more churners, but precision worsens (we capture more false positives).\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a32e2fab-0892-41cb-b33a-343f93ce611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81      1593\n",
      "           1       0.39      0.62      0.48       407\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.64      0.69      0.65      2000\n",
      "weighted avg       0.78      0.73      0.75      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choosing a new threshold\n",
    "optimal_threshold = 0.6  \n",
    "\n",
    "# Apply the new threshold\n",
    "y_pred_adjusted = (y_scores >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate again\n",
    "print(classification_report(y_test, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b2829-05e1-4d93-a44e-489a3cc77744",
   "metadata": {},
   "source": [
    "----\n",
    "#### Increasing the threshold decreases recall, as we capture fewer churners, but precision improves (resulting in fewer false positives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3a92d-80d8-41bf-bf4d-9c66e04e61d5",
   "metadata": {},
   "source": [
    "\n",
    "#### Precision is not sufficient even when recall is lowered. A new model is needed.\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
